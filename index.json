[{"authors":["admin"],"categories":null,"content":"I am currently core-team member at LatticeFlow working on bridging the gap between research and deployment by helping others build and deploy trustworthy and robust machine learning models. I completed my graduate studies at ETH-Zurich, exploring research areas at the intersection of Computer Vision, Machine Learning and Autonomous Driving. I was fortunate to collaborate with people at Andreas Krause's group on learning representations for images with hierarchical labels, Luc van Gool's group on autonomous driving and Roland Siegwart's group on robotics. I spend my time contributing to open-source software. Previously, I was involved in autonomous driving research at nuTonomy (an Aptiv company) and AMZ Driverless. During my undergraduate studies, I researched under James Davis and Rajan Vaish during my sophomore year. I spent a wonderful summer of 2016 collaborating with Wolfram Burgard at the University of Freiburg, Germany funded by a DAAD scholarship. ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ankitdhall.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently core-team member at LatticeFlow working on bridging the gap between research and deployment by helping others build and deploy trustworthy and robust machine learning models. I completed my graduate studies at ETH-Zurich, exploring research areas at the intersection of Computer Vision, Machine Learning and Autonomous Driving.","tags":null,"title":"","type":"authors"},{"authors":["Ankit Dhall","Anastasia Makarova","Octavian Ganea","Dario Pavllo","Michael Greeff","Andreas Krause"],"categories":[],"content":"Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset.\n","date":1586339346,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586339346,"objectID":"c42d30f59341686999b82005ebfe1d7e","permalink":"https://ankitdhall.github.io/publication/hierarchical-image-classification-using-entailment-cone-embeddings/","publishdate":"2020-04-02T11:49:06+02:00","relpermalink":"/publication/hierarchical-image-classification-using-entailment-cone-embeddings/","section":"publication","summary":"Computer Vision and Pattern Recognition (CVPR), DiffCVML, 2020","tags":["machine learning","computer vision","deep learning","geometry","thesis","dataset","riemannian methods for CV"],"title":"Hierarchical Image Classification Using Entailment Cone Embeddings","type":"publication"},{"authors":["Ankit Dhall"],"categories":[],"content":"The work was done in collaboration at Prof. Wolfram Burgard\u0026rsquo;s Autonomous Intelligent Systems, University of Freiburg with Abhinav Valada.\nThe project involves designing an innovative learnable mechanism to perform robust semantic segmentation in adverse conditions. The model learns how to dynamically weigh different modalities and trust them based on their quality resulting in improved segmentation. We also perform experiments on an autonomous car robot, Viona to drive in forested terrain based only on the perception input of the proposed CNN model.\n AdapNet Code and Data The code is available on GitHub and the multi-modal semantic segmentation image dataset used can be found here. The project demo can be found here.\nVideos and Demos  AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions\n Performance on Cityscapes dataset\n Off-road autonomous driving experiments\n CMoDE Code and Data The code is available on GitHub and the multi-modal semantic segmentation image dataset used can be found here. The official project page can be found here.\n ","date":1586336788,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586336788,"objectID":"f227c9e42cd6a1c034e5a0a0504e3fff","permalink":"https://ankitdhall.github.io/project/robust-semantic-segmentation/","publishdate":"2020-04-08T11:06:28+02:00","relpermalink":"/project/robust-semantic-segmentation/","section":"project","summary":" ","tags":["robotics","computer vision","machine learning","deep learning","geometry","autonomous driving","semantic segmentation","dataset","code","ros"],"title":"Robust Semantic Segmentation","type":"project"},{"authors":["Ankit Dhall"],"categories":[],"content":"Publications   Learning Representations for Images With Hierarchical Labels  Hierarchical Image Classification Using Entailment Cone Embeddings   I worked on my Master thesis at Andreas Krause\u0026rsquo;s Learning and Adaptive Systems Group@ETH-Zurich supervised by Anastasia Makarova, Octavian Eugen-Ganea and Dario Pavllo.\n The project works on using external information in the form of leveraging hierarchy formed by labels to aid image classification. The work compares two different types of models: CNN-based models and Euclidean + non-Euclidean embedding-based models.\nCNN-based models Instead of proposing special modules, the CNN-based models exploit the hierarchy using loss functions that incorporate this hierarchical information in different degrees.\nEuclidean and Non-Euclidean embeddings Euclidean and non-Euclidean embedding models are taken from natural language processing (NLP) where they are hugely prevalent. We propose to use these models for computer vision and images by learning representation in joint-embedding spaces for both concepts and images.\n Predicting Taxonomy for Organisms One of the main applications of this work is to assist natural history collections, museums and organizations that preserve large numbers of historical and extant biodiversity specimens to digitize and organize their collections. Hobbyists create their personal collections most of which are eventually donated to public institutions. Before integration, these specimens need to be sorted taxonomically by specialists who have little time and are expensive. If this resource intensive task could be preceded by a pre-sorting procedure, for instance, where these specimens are categorized by unskilled labour based on their family, sub-family, genus, and species it would expedite and economize the process.\nThanks to the The ETH Library Lab and Michael Greeff the research conducted on the thesis will be turned into classification app that can be used by hobbyists, collectors, and researchers alike to speed up and economize classification and segregation of entomological specimens. More information about the app will be made available soon!\n","date":1586277109,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586277109,"objectID":"b628c1106936756165f2706414647c93","permalink":"https://ankitdhall.github.io/project/learning-representations-for-images-with-hierarchical-labels/","publishdate":"2020-04-07T18:31:49+02:00","relpermalink":"/project/learning-representations-for-images-with-hierarchical-labels/","section":"project","summary":" ","tags":["machine learning","computer vision","deep learning","geometry","thesis","dataset","riemannian methods for CV"],"title":"Representation Learning","type":"project"},{"authors":["Ankit Dhall"],"categories":[],"content":"Image classification has been studied extensively but there has been limited work in the direction of using non-conventional, external guidance other than traditional image-label pairs to train such models. In this thesis we present a set of methods to leverage information about the semantic hierarchy induced by class labels. In the first part of the thesis, we inject label-hierarchy knowledge to an arbitrary classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions by using order-preserving embedding-based models, prevalent in natural language, and tailor them to the domain of computer vision to perform image classification. Although, contrasting in nature, both the CNN-classifiers injected with hierarchical information, and the embedding-based models outperform a hierarchy-agnostic model on the newly presented, real-world ETH Entomological Collection image dataset.\n","date":1586267129,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586267129,"objectID":"a64a39e45c09d41cefa56d5f17a23d1c","permalink":"https://ankitdhall.github.io/publication/learning-representations-for-images-with-hierarchical-labels/","publishdate":"2020-04-07T15:45:29+02:00","relpermalink":"/publication/learning-representations-for-images-with-hierarchical-labels/","section":"publication","summary":"Master Thesis, 2019","tags":["machine learning","computer vision","deep learning","geometry","thesis","dataset","riemannian methods for CV"],"title":"Learning Representations for Images With Hierarchical Labels","type":"publication"},{"authors":["Ankit Dhall"],"categories":[],"content":"Keypoint-Network for Monocular Pose Estimation The work was done in collaboration with CVL@ETH-Zurich with Dengxin Dai and Prof. Luc van Gool. The official project page can be found here.\n  AMZ-Driverless I was officially involved in the 2018-2019 season as the Head of Perception with pilatus driverless. The year before, I was worked on the computer vision pipeline, more specifically developed the monocular pipeline and the keypoint network.\nIn 2018 we were declared Champions at Formula Student Italy and Formula Student Germany and in 2019 at Formula Student East and Formula Student Germany.Here\u0026rsquo;s a video of \u0026ldquo;gotthard driverless\u0026rdquo; cruising autonomously at the Hockenheimring from FSG'18. \nInvolvement pilatus driverless: Head of Perception [2018-2019]\ngotthard driverless: Computer Vision [2017-2018]\n Driverless cars in action pilatus driverless @FS Germany 2019   Acceleration  Trackdrive  AutoX  gotthard driverless @FS Germany 2018   Trackdrive   Acknowledgement We would like to thank AMZ Racing, especially the Driverless team without whom this project would not have been possible. The work is also supported by Toyota Motor Europe via the project TRACE-Zurich.\n","date":1586161480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586161480,"objectID":"d9aca30a0dad339ed7d03bee4eb72f87","permalink":"https://ankitdhall.github.io/project/keypoint-network-amz/","publishdate":"2020-04-06T10:24:40+02:00","relpermalink":"/project/keypoint-network-amz/","section":"project","summary":" ","tags":["robotics","computer vision","machine learning","deep learning","geometry","autonomous driving","depth estimation"],"title":"AMZ Driverless","type":"project"},{"authors":["Ankit Dhall"],"categories":[],"content":"","date":1586096175,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586096175,"objectID":"670a35a85be678ee2d7e5df02134bb46","permalink":"https://ankitdhall.github.io/project/lidar-camera-calibration/","publishdate":"2020-04-05T16:16:15+02:00","relpermalink":"/project/lidar-camera-calibration/","section":"project","summary":" ","tags":["robotics","perception","lidar","camera","ros","tools","calibration","computer vision"],"title":"Lidar Camera Calibration","type":"project"},{"authors":["Juraj Kabzan","Miguel de la Iglesia Valls","Victor Reijgwart","Hubertus FC Hendrikx","Claas Ehmke","Manish Prajapat","Andreas Bühler","Nikhil Gosala","Mehak Gupta","Ramya Sivanesan","Ankit Dhall","Eugenio Chisari","Napat Karnchanachari","Sonja Brits","Manuel Dangel","Inkyu Sa","Renaud Dubé","Abel Gawel","Mark Pfeiffer","Alexander Liniger","John Lygeros","Roland Siegwart"],"categories":[],"content":"This paper presents the algorithms and system architecture of an autonomous racecar. The introduced vehicle is powered by a software stack designed for robustness, reliability, and extensibility. In order to autonomously race around a previously unknown track, the proposed solution combines state of the art techniques from different fields of robotics. Specifically, perception, estimation, and control are incorporated into one high-performance autonomous racecar. This complex robotic system, developed by AMZ Driverless and ETH Zurich, finished 1st overall at each competition we attended: Formula Student Germany 2017, Formula Student Italy 2018 and Formula Student Germany 2018. We discuss the findings and learnings from these competitions and present an experimental evaluation of each module of our solution.\n","date":1557331618,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557331618,"objectID":"f5291c090371c8e56276e87a2f86333c","permalink":"https://ankitdhall.github.io/publication/amz-driverless-full-autonomous-racing/","publishdate":"2019-05-01T18:06:58+02:00","relpermalink":"/publication/amz-driverless-full-autonomous-racing/","section":"publication","summary":"arXiv preprint, 2019","tags":["robotics","computer vision","machine learning","deep learning","geometry","autonomous driving","depth estimation"],"title":"AMZ Driverless: The Full Autonomous Racing System","type":"publication"},{"authors":["Ankit Dhall","Dengxin Dai","Luc Van Gool"],"categories":[],"content":"Considerable progress has been made in semantic scene understanding of road scenes with monocular cameras. It is, however, mainly focused on certain specific classes such as cars, bicyclists and pedestrians. This work investigates traffic cones, an object category crucial for traffic control in the context of autonomous vehicles. 3D object detection using images from a monocular camera is intrinsically an ill-posed problem. In this work, we exploit the unique structure of traffic cones and propose a pipelined approach to solve this problem. Specifically, we first detect cones in images by a modified 2D object detector. Following which the keypoints on a traffic cone are recognized with the help of our deep structural regression network, here, the fact that the cross-ratio is projection invariant is leveraged for network regularization. Finally, the 3D position of cones is recovered via the classical Perspective n-Point algorithm using correspondences obtained from the keypoint regression. Extensive experiments show that our approach can accurately detect traffic cones and estimate their position in the 3D world in real time. The proposed method is also deployed on a real-time, autonomous system. It runs efficiently on the low-power Jetson TX2, providing accurate 3D position estimates, allowing a race-car to map and drive autonomously on an unseen track indicated by traffic cones. With the help of robust and accurate perception, our race-car won both Formula Student Competitions held in Italy and Germany in 2018, cruising at a top speed of 54 km/h on our driverless platform ``gotthard driverless'' ","date":1549441467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549441467,"objectID":"99c1b3e22d8aea78186f88c64967872f","permalink":"https://ankitdhall.github.io/publication/keypoint-network-amz/","publishdate":"2019-02-06T10:24:27+02:00","relpermalink":"/publication/keypoint-network-amz/","section":"publication","summary":"IEEE Intelligent Vehicles (IV), Paris 2019","tags":["robotics","computer vision","machine learning","deep learning","geometry","autonomous driving","depth estimation"],"title":"Real-time 3D Traffic Cone Detection for Autonomous Driving","type":"publication"},{"authors":["Ankit Dhall"],"categories":[],"content":"We propose a complete pipeline that allows object detection and simultaneously estimate the pose of these multiple object instances using just a single image. A novel \u0026ldquo;keypoint regression\u0026rdquo; scheme with a cross-ratio term is introduced that exploits prior information about the object\u0026rsquo;s shape and size to regress and find specific feature points. Further, a priori 3D information about the object is used to match 2D-3D correspondences and accurately estimate object positions up to a distance of 15m. A detailed discussion of the results and an in-depth analysis of the pipeline is presented. The pipeline runs efficiently on a low-powered Jetson TX2 and is deployed as part of the perception pipeline on a real-time autonomous vehicle cruising at a top speed of 54 km/hr.\n","date":1536422846,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536422846,"objectID":"f7b0c6a7c078960f476d3e65e29c9b88","permalink":"https://ankitdhall.github.io/publication/real-time-3d-monocular-pose-estimation/","publishdate":"2018-09-08T18:07:26+02:00","relpermalink":"/publication/real-time-3d-monocular-pose-estimation/","section":"publication","summary":"arXiv preprint, 2019","tags":["robotics","computer vision","machine learning","deep learning","geometry","autonomous driving","depth estimation"],"title":"Real-time 3D Pose Estimation with a Monocular Camera Using Deep Learning and Object Priors","type":"publication"},{"authors":["Ankit Dhall","Kunal Chelani","Vishnu Radhakrishnan","K.M. Krishna"],"categories":[],"content":"Usage Tutorials:   \nROS package\n\nStar Fork \nWith the advent of autonomous vehicles, LiDAR and cameras have become an indispensable combination of sensors. They both provide rich and complementary data which can be used by various algorithms and machine learning to sense and make vital inferences about the surroundings. We propose a novel pipeline and experimental setup to find accurate rigid-body transformation for extrinsically calibrating a LiDAR and a camera. The pipeling uses 3D-3D point correspondences in LiDAR and camera frame and gives a closed form solution. We further show the accuracy of the estimate by fusing point clouds from two stereo cameras which align perfectly with the rotation and translation estimated by our method, confirming the accuracy of our method's estimates both mathematically and visually. Taking our idea of extrinsic LiDAR-camera calibration forward, we demonstrate how two cameras with no overlapping field-of-view can also be calibrated extrinsically using 3D point correspondences. The code has been made available as open-source software in the form of a ROS package. ![alt text](lidar_cam_calib.gif \"After sticting point clouds from wide-angled stereo using our lidar-camera-calibration ROS package\") ","date":1493992879,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493992879,"objectID":"1cbb4156c4fe7d0c9a1c742ac5c32900","permalink":"https://ankitdhall.github.io/publication/lidar-camera-calibration/","publishdate":"2017-05-05T12:00:00+02:00","relpermalink":"/publication/lidar-camera-calibration/","section":"publication","summary":"arXiv preprint, 2017","tags":["robotics","perception","lidar","camera","ros","tools","calibration"],"title":"LiDAR-Camera Calibration using 3D-3D Point correspondences","type":"publication"},{"authors":["Abhinav Valada","Johan Vertens","Ankit Dhall","Wolfram Burgard"],"categories":[],"content":"project page + web demo | videos [1] [2] [3] \nRobust scene understanding of outdoor environments using passive optical sensors is an onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometers of a forested environment (see the video links for more info) using only the segmentation for perception.\n","date":1491427548,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491427548,"objectID":"809abce80fa4787b1bd13512725bb417","permalink":"https://ankitdhall.github.io/publication/adapnet/","publishdate":"2020-04-05T23:25:48+02:00","relpermalink":"/publication/adapnet/","section":"publication","summary":" IEEE International Conference on Robotics and Automation, Singapore 2017","tags":[],"title":"AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions","type":"publication"},{"authors":["Ankit Dhall","Abhinav Valada","Wolfram Burgard"],"categories":[],"content":"In this paper, we propose Convoluted Mixture of Deep Experts (CMoDE) model that enables a multi-stream deep neural network architecture to learn features from complementary modalities and spectra that are resilient to commonly observed environmental disturbances. Some of these disturbances include shadows, snow, rain and glare which vary depending with season and time of the day. Our model first adaptively weighs features from each of the individual experts and then further learns fused representations that are robust to these disturbances. We comprehensively evaluate the CMoDE model against several other existing fusion approaches and show that our proposed model exceeds the state-of-the-art.\n","date":1459892460,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459892460,"objectID":"e02fe8d8df625336e2b7d44101910cda","permalink":"https://ankitdhall.github.io/publication/cmode/","publishdate":"2016-04-05T23:41:00+02:00","relpermalink":"/publication/cmode/","section":"publication","summary":"IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)-W, S. Korea 2016","tags":["robotics","computer vision","machine learning","deep learning","geometry","autonomous driving","semantic segmentation","dataset","code","ros"],"title":"Convoluted Mixture of Deep Experts for Robust Semantic Segmentation","type":"publication"},{"authors":["Andreas Veit","Michael Wilber","Rajan Vaish","Serge Belongie","James Davis","...","Ankit Dhall","et al."],"categories":[],"content":"When crowdsourcing systems are used in combination with machine inference systems in the real world, they benefit the most when the machine system is deeply integrated with the crowd workers. However, if researchers wish to integrate the crowd with \u0026ldquo;off-the-shelf\u0026rdquo; machine classifiers, this deep integration is not always possible. This work explores two strategies to increase accuracy and decrease cost under this setting. First, we show that reordering tasks presented to the human can create a significant accuracy improvement. Further, we show that greedily choosing parameters to maximize machine accuracy is sub-optimal, and joint optimization of the combined system improves performance.\n","date":1428270506,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428270506,"objectID":"8e48ef86fc3debb96a554a70d5a5a3ea","permalink":"https://ankitdhall.github.io/publication/optimizing-human-machine-task-assignment/","publishdate":"2020-04-05T23:48:26+02:00","relpermalink":"/publication/optimizing-human-machine-task-assignment/","section":"publication","summary":"AAAI Conference on Human Computation and Crowdsourcing (HCOMP)-W 2015","tags":["hci","computer vision","crowdworking","crowdsourcing"],"title":"On Optimizing Human-Machine Task Assignments","type":"publication"}]