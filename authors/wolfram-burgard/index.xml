<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wolfram Burgard | Ankit Dhall</title>
    <link>https://ankitdhall.github.io/authors/wolfram-burgard/</link>
      <atom:link href="https://ankitdhall.github.io/authors/wolfram-burgard/index.xml" rel="self" type="application/rss+xml" />
    <description>Wolfram Burgard</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Ankit Dhall</copyright><lastBuildDate>Wed, 05 Apr 2017 23:25:48 +0200</lastBuildDate>
    <image>
      <url>https://ankitdhall.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Wolfram Burgard</title>
      <link>https://ankitdhall.github.io/authors/wolfram-burgard/</link>
    </image>
    
    <item>
      <title>AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions</title>
      <link>https://ankitdhall.github.io/publication/adapnet/</link>
      <pubDate>Wed, 05 Apr 2017 23:25:48 +0200</pubDate>
      <guid>https://ankitdhall.github.io/publication/adapnet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34; target=&#34;_blank&#34;&gt;project page + web demo&lt;/a&gt; |
videos &lt;a href=&#34;https://www.youtube.com/watch?v=E6gij6IS8n0&#34; title=&#34;AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=7teAwVMTCho&#34; title=&#34;cityscapes demo&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=cV-k0gkVfuY&#34; title=&#34;autonomous navigation experiments&#34; target=&#34;_blank&#34;&gt;[3]&lt;/a&gt; &lt;p&gt;&lt;/p&gt;
Robust scene understanding of outdoor environments using passive optical sensors is an onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometers of a forested environment (see the video links for more info) using only the segmentation for perception.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convoluted Mixture of Deep Experts for Robust Semantic Segmentation</title>
      <link>https://ankitdhall.github.io/publication/cmode/</link>
      <pubDate>Tue, 05 Apr 2016 23:41:00 +0200</pubDate>
      <guid>https://ankitdhall.github.io/publication/cmode/</guid>
      <description>&lt;p&gt;In this paper, we propose Convoluted Mixture of Deep Experts (CMoDE) model that enables a multi-stream deep neural network architecture to learn features from complementary modalities and spectra that are resilient to commonly observed environmental disturbances. Some of these disturbances include shadows, snow, rain and glare which vary depending with season and time of the day. Our model first adaptively weighs features from each of the individual experts and then further learns fused representations that are robust to these disturbances. We comprehensively evaluate the CMoDE model against several other existing fusion approaches and show that our proposed model exceeds the state-of-the-art.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
