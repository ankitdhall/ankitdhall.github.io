<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Source from Jon Barron and Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <title>Ankit Dhall</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Ankit Dhall</name>
        </p>
        <p align="justify">I work on Computer Vision and Machine Learning at <a href="https://seervision.com" target="_blank">Seervision</a>. I completed my graduate studies at <a href="https://ethz.ch/en.html" target="_blank">ETH-Zurich</a>, exploring research areas at the intersection of Computer Vision and Machine Learning.
        <br><br>
        I worked on my Master thesis at <a href="https://las.inf.ethz.ch/krausea" target="_blank">Prof. Andreas Krause's</a> group, <a href="https://las.inf.ethz.ch/" target="_blank">Learning & Adaptive Systems</a> on learning representations for images with hierarchical labels under the supervision of <a href="https://las.inf.ethz.ch/people/anastasia-makarova" target="_blank">Anastasia Makarova</a>, <a href="http://people.inf.ethz.ch/ganeao/" target="_blank">Octavian Eugen-Ganea</a> and <a href="http://da.inf.ethz.ch/people/DarioPavllo/" target="_blank">Dario Pavllo</a>.
        <br><br>
        Previously, I was a research intern at <a href="https://www.nutonomy.com/" target="_blank">nuTonomy (an Aptiv company)</a>, Singapore working on perception and machine learning for autonomous driving. 
        <br><br>
        I was the Head of Perception at <a href="https://www.amzracing.ch/" target="_blank">Academic Motorsport Association of Zurich (AMZ)</a> for the Formula Student Driverless, Season 2018/2019. In 2017/2018, I contributed to the Computer Vision package.
        <br>
        In 2018 we were declared Champions at <a href="https://www.formula-ata.it/" target="_blank">Formula Student Italy</a> and <a href="https://www.formulastudent.de/" target="_blank">Formula Student Germany</a> and in 2019 at Formula Student East and <a href="https://www.formulastudent.de/" target="_blank">Formula Student Germany</a>.
        <br><br> <a href="https://youtu.be/HegmIXASKow?t=11694" target="_blank"> Here's a video of "gotthard driverless" cruising autonomously at the Hockenheimring from FSG'18. </a>
        <br><br>
        In 2015, I had the opportunity to work with <a href="https://users.soe.ucsc.edu/~davis/" target="_blank">Prof. James Davis</a> and <a href="http://www.rajanvaish.com/index.html" target="_blank">Rajan Vaish</a> during my sophomore year. I spent a wonderful summer of 2016 with <a href="http://www2.informatik.uni-freiburg.de/~burgard/" target="_blank">Prof. Wolfram Burgard's</a> <a href="http://ais.informatik.uni-freiburg.de/" target="_blank">research group</a> at the University of Freiburg, Germany funded by <a href="https://www.daad.de/en/" target="_blank">DAAD</a>. I completed my Bachelor's at VIT University studying Computer Science under the guidance of Prof. Nithyadarsini and Prof. Sivagami. I completed my thesis at <a href="http://faculty.iiit.ac.in/~mkrishna/index.html" target="_blank">Prof. K.M. Krishna's</a> lab at Robotics Research Center, IIIT-Hyderabad.
          
        </p>
        <p align=center>
          <a href="mailto:removethisifhuman-adhall@ethz.ch">Email</a> &nbsp/&nbsp
          <a href="/assets/Dhall_Ankit.pdf" target="_blank">CV</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=h4rehzUAAAAJ" target="_blank">Google Scholar</a> &nbsp/&nbsp 
          <a href="http://www.linkedin.com/in/ankitdhall/" target="_blank"> LinkedIn </a> &nbsp/&nbsp
          <a href="https://github.com/ankitdhall" target="_blank">Github</a> 
        </p>
        </td>
        <td width="33%">
        <img src="/assets/sq.jpg" width="240px" style="border-radius:42%">
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research + Projects</heading>
          <p>
          I'm interested in exploring research areas at the intersection of Machine Learning and Computer Vision.
          </p>
        </td>
      </tr>
      </table>
  <script>
    function myFunction(pub_name) {
        var x = document.getElementById(pub_name);
        if (x.style.display === 'none') {
            x.style.display = 'block';
        } else {
            x.style.display = 'none';
        }
  }
  </script>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr>
      <td width="25%">
        <img src='/assets/publications_img/FSG19_Trackdrive_D.gif'  style="min-width:250px;" width="48%">
      </td>
      <td valign="top" width="75%">
        <p><a href="http://driverless.amzracing.ch/" target="_blank">
        <papertitle> AMZ Driverless </papertitle></a><br>

          <font color="red"><em>pilatus driverless</em></font>: Head of Perception [2018-2019]<br>
          <font color="red"><em>gotthard driverless</em></font>: Computer Vision [2017-2018]<br>

        <p></p>
        <font color="red"><em>pilatus driverless</em></font>@FS Germany 2019:
        <a href="https://youtu.be/OX8RSyF9mnw?list=PLtuNXpGOPQ_b5fejuHQrdE8PcW5kJ31F7&t=10079">Acceleration</a> |
        <a href="https://youtu.be/gcnngFyWnFQ?list=PLtuNXpGOPQ_b5fejuHQrdE8PcW5kJ31F7&t=13068" target="_blank">Trackdrive</a> |
        <a href="https://youtu.be/sxqkt_ydOkY?list=PLtuNXpGOPQ_b5fejuHQrdE8PcW5kJ31F7&t=10749" target="_blank">AutoX</a> <br>

        <font color="red"><em>gotthard driverless</em></font>@FS Germany 2018:
        <a href="https://youtu.be/HegmIXASKow?t=11694" target="_blank">Trackdrive</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='/assets/publications_img/gotthard_pipeline.png'  style="min-width:250px;" width="48%">
      </td>
      <td valign="top" width="75%">
        <p><a href="" target="_blank">
        <!-- <papertitle><img src="/assets/new.png" height="15%" /> Real-time 3D Traffic Cone Detection for Autonomous Driving</papertitle></a><br> -->
        <papertitle>Real-time 3D Traffic Cone Detection for Autonomous Driving</papertitle></a><br>

          <strong>Ankit Dhall</strong>, <a href="http://www.vision.ee.ethz.ch/~daid/" target="_blank">Dengxin Dai</a>, <a href="http://www.vision.ee.ethz.ch/en/members/get_member.cgi?lang=en&id=1" target="_blank">Luc Van Gool</a>
          <br>

        <em>IEEE Intelligent Vehicles (IV)</em>, Paris 2019 <br>
        <p></p>
        <a href="javascript:void(0);" onclick="myFunction('gotthard')">abstract</a> |
        <a href="https://www.trace.ethz.ch/TrafficCone/" target="_blank">project</a> |
        <a href="https://arxiv.org/abs/1902.02394" target="_blank">arXiv</a>
        <p></p>

        <div id="gotthard" style="display:none; text-align:justify;min-width:350px;">
            Considerable progress has been made in semantic scene understanding of road scenes with monocular cameras. It is, however, mainly focused on certain specific classes such as cars, bicyclists and pedestrians. This work investigates traffic cones, an object category crucial for traffic control in the context of autonomous vehicles. 3D object detection using images from a monocular camera is intrinsically an ill-posed problem. In this work, we exploit the unique structure of traffic cones and propose a pipelined approach to solve this problem. Specifically, we first detect cones in images by a modified 2D object detector. Following which the keypoints on a traffic cone are recognized with the help of our deep structural regression network, here, the fact that the cross-ratio is projection invariant is leveraged for network regularization. Finally, the 3D position of cones is recovered via the classical Perspective n-Point algorithm using correspondences obtained from the keypoint regression.
            Extensive experiments show that our approach can accurately detect traffic cones and estimate their position in the 3D world in real time. The proposed method is also deployed on a real-time, autonomous system. It runs efficiently on the low-power Jetson TX2, providing accurate 3D position estimates, allowing a race-car to map and drive autonomously on an unseen track indicated by traffic cones. With the help of robust and accurate perception, our race-car won both Formula Student Competitions held in Italy and Germany in 2018, cruising at a top speed of 54 km/h on our driverless platform ``gotthard driverless''
        </div>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='/assets/publications_img/lidar_cam_calib.gif'  style="min-width:250px;" width="48%">
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/pdf/1705.09785" target="_blank">
        <papertitle> LiDAR-Camera Calibration using 3D-3D Point correspondences</papertitle></a><br>

          <strong>Ankit Dhall</strong>, Kunal Chelani, Vishnu Radhakrishnan, <a href="http://faculty.iiit.ac.in/~mkrishna/index.html" target="_blank">K.M. Krishna</a>
          <br>

        <em>arXiv pre-print</em>, 2017 <br>
        <p></p>
        <a href="javascript:void(0);" onclick="myFunction('lcc')">abstract</a> |
        <a href="https://arxiv.org/abs/1705.09785" target="_blank">arXiv</a> |
        <a href="http://wiki.ros.org/lidar_camera_calibration" target="_blank">ROS package</a> |
        <a href="https://github.com/ankitdhall/lidar_camera_calibration" target="_blank">GitHub repo</a> |
        videos <a href="https://youtu.be/SiPGPwNKE-Q" title="Tutorial on how to use the lidar_camera_calibration ROS package" target="_blank">[1]</a>
        <a href="https://youtu.be/Om1SFPAZ5Lc" title="Fused point cloud with two stereo cameras kept at ~80 degrees" target="_blank">[2]</a>
        <a href="https://youtu.be/AbjRDtHLdz0" title="Fusing point clouds using manual measurement vs. lidar_camera_calibration" target="_blank">[3]</a>
        <p></p>

        <div id="lcc" style="display:none; text-align:justify;min-width:350px;">
            With the advent of autonomous vehicles, LiDAR and cameras have become an indispensable combination of sensors. They both provide rich and complementary data which can be used by various algorithms and machine learning to sense and make vital inferences about the surroundings. We propose a novel pipeline and experimental setup to find accurate rigid-body transformation for extrinsically calibrating a LiDAR and a camera. The pipeling uses 3D-3D point correspondences in LiDAR and camera frame and gives a closed form solution. We further show the accuracy of the estimate by fusing point clouds from two stereo cameras which align perfectly with the rotation and translation estimated by our method, confirming the accuracy of our method's estimates both mathematically and visually. Taking our idea of extrinsic LiDAR-camera calibration forward, we demonstrate how two cameras with no overlapping field-of-view can also be calibrated extrinsically using 3D point correspondences. The code has been made available as open-source software in the form of a ROS package.
        </div>
      </td>
    </tr>
	 
    <tr>
      <td width="25%">
        <img src='/assets/publications_img/icra17_adapnet1.gif'  style="min-width:250px;" width="48%">
      </td>
      <td valign="top" width="75%">
        <p><a href="/assets/papers/AdapNet_icra17.pdf" target="_blank">
        <papertitle>AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions</papertitle></a><br>
          <a href="http://www2.informatik.uni-freiburg.de/~valada/" target="_blank">Abhinav Valada</a>, 
          <a href="http://www2.informatik.uni-freiburg.de/~vertensj/" target="_blank">Johan Vertens</a>,
          <strong>Ankit Dhall</strong>,
          <a href="http://www2.informatik.uni-freiburg.de/~burgard/" target="_blank">Wolfram Burgard</a>
          <br>

        <em>ICRA</em>, 2017 <br>
        <p></p>

        <a href="javascript:void(0);" onclick="myFunction('icra17')">abstract</a> |
        <a href="http://deepscene.cs.uni-freiburg.de/" target="_blank">project page + web demo</a> |
        videos <a href="https://www.youtube.com/watch?v=E6gij6IS8n0" title="AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions" target="_blank">[1]</a>
        <a href="https://www.youtube.com/watch?v=7teAwVMTCho" title="cityscapes demo" target="_blank">[2]</a>
        <a href="https://www.youtube.com/watch?v=cV-k0gkVfuY" title="autonomous navigation experiments" target="_blank">[3]</a>
        <p></p>

        <div id="icra17" style="display:none; text-align:justify;min-width:350px;">
            Robust scene understanding of outdoor environments using passive optical sensors is an onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometers of a forested environment (see the video links for more info) using only the segmentation for perception.
        </div>
      </td>
    </tr>
    
    <tr>
      <td width="25%">
        <img src='/assets/publications_img/iros16_ws.gif'  style="min-width:250px;" width="48%">
      </td>
      <td valign="top" width="75%">
        <p><a href="/assets/papers/CMoDE_iros16.pdf" target="_blank">
        <papertitle>Convoluted Mixture of Deep Experts for Robust Semantic Segmentation</papertitle></a><br>
          <a href="http://www2.informatik.uni-freiburg.de/~valada/" target="_blank">Abhinav Valada*</a>,
          <strong>Ankit Dhall*</strong>,
          <a href="http://www2.informatik.uni-freiburg.de/~burgard/" target="_blank">Wolfram Burgard</a>
          <br>
          
        <em>State Estimation and Terrain Perception for All Terrain Mobile Robots Workshop at IROS</em>, 2016 <br>
        <p></p>

        <a href="javascript:void(0);" onclick="myFunction('iros16')">abstract</a> |
        <a href="http://deepscene.cs.uni-freiburg.de" target="_blank">project page + web demo</a>
        <p></p>

        <div id="iros16" style="display:none; text-align:justify;min-width:350px;">
            In this paper, we propose Convoluted Mixture of Deep Experts (CMoDE) model that enables a multi-stream deep neural network architecture to learn features from complementary modalities and spectra that are resilient to commonly observed environmental disturbances. Some of these disturbances include shadows, snow, rain and glare which vary depending with season and time of the day. Our model first adaptively weighs features from each of the individual experts and then further learns fused representations that are robust to these disturbances. We comprehensively evaluate the CMoDE model against several other existing fusion approaches and show that our proposed model exceeds the state-of-the-art.
        </div>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='/assets/publications_img/hcomp15.gif'  style="min-width:250px;" width="48%">
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/pdf/1509.07543" target="_blank">
        <papertitle>On Optimizing Human-Machine Task Assignments</papertitle></a><br>
          <a href="https://www.cs.cornell.edu/~andreas/" target="_blank">Andreas Veit</a>,
          <a href="http://mjwilber.org/" target="_blank">Michael Wilber</a>,
          <a href="http://www.rajanvaish.com/index.html" target="_blank">Rajan Vaish</a>,
          <a href="https://vision.cornell.edu/se3/people/serge-belongie/" target="_blank">Serge Belongie</a>,
          <a href="https://users.soe.ucsc.edu/~davis/" target="_blank">James Davis</a>, ...,
          <strong>Ankit Dhall</strong>,
          et al.
          <br>
          
        <em>HCOMP</em>, 2015 <br>
        <p></p>

        <a href="javascript:void(0);" onclick="myFunction('hcomp15')">abstract</a> |
        <a href="https://arxiv.org/abs/1509.07543" target="_blank">arXiv</a>
        <p></p>

        <div id="hcomp15" style="display:none; text-align:justify;min-width:350px;">
            When crowdsourcing systems are used in combination with machine inference systems in the real world, they benefit the most when the machine system is deeply integrated with the crowd workers. However, if researchers wish to integrate the crowd with "off-the-shelf" machine classifiers, this deep integration is not always possible. This work explores two strategies to increase accuracy and decrease cost under this setting. First, we show that reordering tasks presented to the human can create a significant accuracy improvement. Further, we show that greedily choosing parameters to maximize machine accuracy is sub-optimal, and joint optimization of the combined system improves performance.
        </div>
      </td>
    </tr>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          <a href="https://jonbarron.info/" target="_blank">Template credits: Jon Barron.</a>
      </font>
        </p>
        </td>
      </tr>
      </table>

  <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-91292266-1', 'auto');
      ga('send', 'pageview');

    </script>
  </table>
  </body>
</html>
