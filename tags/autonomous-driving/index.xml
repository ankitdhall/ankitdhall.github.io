<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>autonomous driving | Ankit Dhall</title>
    <link>https://ankitdhall.github.io/tags/autonomous-driving/</link>
      <atom:link href="https://ankitdhall.github.io/tags/autonomous-driving/index.xml" rel="self" type="application/rss+xml" />
    <description>autonomous driving</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Ankit Dhall</copyright><lastBuildDate>Wed, 08 Apr 2020 11:06:28 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>autonomous driving</title>
      <link>https://ankitdhall.github.io/tags/autonomous-driving/</link>
    </image>
    
    <item>
      <title>Robust Semantic Segmentation</title>
      <link>https://ankitdhall.github.io/project/robust-semantic-segmentation/</link>
      <pubDate>Wed, 08 Apr 2020 11:06:28 +0200</pubDate>
      <guid>https://ankitdhall.github.io/project/robust-semantic-segmentation/</guid>
      <description>&lt;p&gt;The work was done in collaboration at 
&lt;a href=&#34;http://www2.informatik.uni-freiburg.de/~burgard&#34; title=&#34;Prof. Wolfram Burgard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Wolfram Burgard&amp;rsquo;s&lt;/a&gt; 
&lt;a href=&#34;http://ais.informatik.uni-freiburg.de/index_en.php&#34; title=&#34;AIS, Uni-Freiburg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Autonomous Intelligent Systems, University of Freiburg&lt;/a&gt; with 
&lt;a href=&#34;http://www2.informatik.uni-freiburg.de/~valada/&#34; title=&#34;Abhinav Valada&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abhinav Valada&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The project involves designing an innovative learnable mechanism to perform robust semantic segmentation in adverse conditions. The model learns how to dynamically weigh different modalities and trust them based on their quality resulting in improved segmentation. We also perform experiments on an autonomous car robot, Viona to drive in forested terrain based only on the perception input of the proposed CNN model.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;adapnet&#34;&gt;AdapNet&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;adapnet.gif&#34; alt=&#34;alt text&#34; title=&#34;Qualitative results under various adverse conditions like rain, nigth and fall scenarios from the dataset.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;icra17_adapnet1.gif&#34; alt=&#34;alt text&#34; title=&#34;AdapNet CNN architecture.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h3&gt;
&lt;p&gt;The code is available on 
&lt;a href=&#34;https://github.com/DeepSceneSeg/AdapNet&#34; title=&#34;AdapNet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and the multi-modal semantic segmentation image dataset used can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/#datasets&#34; title=&#34;Multi-modal semantic segmentation image dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The project demo can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34; title=&#34;Deep-Scene&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;unimodal_vs_cmode.png&#34; alt=&#34;alt text&#34; title=&#34;Comparing performance of uni-modal input to multi-modal input. AdapNet learns better by fusing smartly information across modalities&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;videos-and-demos&#34;&gt;Videos and Demos&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=E6gij6IS8n0&#34; title=&#34;AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=7teAwVMTCho&#34; title=&#34;Performance on Cityscapes dataset&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; Performance on Cityscapes dataset&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=cV-k0gkVfuY&#34; title=&#34;Off-road autonomous driving experiments&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; Off-road autonomous driving experiments&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;autonomous_exp.png&#34; alt=&#34;alt text&#34; title=&#34;Autonomous driving in off-road terrain in adverse conditions. Perception based solely on semantic segmentation output from CMoDE.&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;cmode&#34;&gt;CMoDE&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;qual-results.png&#34; alt=&#34;alt text&#34; title=&#34;Qualitative results with RGB and EVI/Depth as input and their corresponding weights that the CMoDE generates and uses based on image quality to produced improved semantic semgentation as compared to an agnostic model (LFC).&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;segmentation-video.gif&#34; alt=&#34;alt text&#34; title=&#34;Autonomous driving in off-road terrain in adverse conditions. Perception based solely on semantic segmentation output from CMoDE.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;qual_results2.png&#34; alt=&#34;alt text&#34; title=&#34;Qualitative results in different conditions.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;code-and-data-1&#34;&gt;Code and Data&lt;/h3&gt;
&lt;p&gt;The code is available on 
&lt;a href=&#34;https://github.com/DeepSceneSeg/CMoDE&#34; title=&#34;CMoDE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and the multi-modal semantic segmentation image dataset used can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/#datasets&#34; title=&#34;Multi-modal semantic segmentation image dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The official project page can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34; title=&#34;Deep-Scene&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>AMZ Driverless</title>
      <link>https://ankitdhall.github.io/project/keypoint-network-amz/</link>
      <pubDate>Mon, 06 Apr 2020 10:24:40 +0200</pubDate>
      <guid>https://ankitdhall.github.io/project/keypoint-network-amz/</guid>
      <description>&lt;h2 id=&#34;keypoint-network-for-monocular-pose-estimation&#34;&gt;Keypoint-Network for Monocular Pose Estimation&lt;/h2&gt;
&lt;p&gt;The work was done in collaboration with 
&lt;a href=&#34;https://vision.ee.ethz.ch/&#34; title=&#34;Computer Vision Lab, ETH Zurich&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CVL@ETH-Zurich&lt;/a&gt; with 
&lt;a href=&#34;http://www.vision.ee.ethz.ch/~daid/&#34; title=&#34;Dengxin Dai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dengxin Dai&lt;/a&gt; and 
&lt;a href=&#34;https://scholar.google.ch/citations?user=TwMib_QAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34; title=&#34;Prof. Luc van Gool&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Luc van Gool&lt;/a&gt;. The official project page can be found 
&lt;a href=&#34;https://www.trace.ethz.ch/TrafficCone/&#34; title=&#34;Real-time 3D Traffic Cone Detection for Autonomous Driving&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id=&#34;amz-driverlesshttpdriverlessamzracingch-amz-driverless-homepage&#34;&gt;
&lt;a href=&#34;http://driverless.amzracing.ch/&#34; title=&#34;AMZ-Driverless&amp;#39; Homepage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMZ-Driverless&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;I was officially involved in the 2018-2019 season as the Head of Perception with &lt;strong&gt;pilatus driverless&lt;/strong&gt;. The year before, I was worked on the computer vision pipeline, more specifically developed the monocular pipeline and the keypoint network.&lt;/p&gt;
&lt;h2 id=&#34;involvement&#34;&gt;Involvement&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;pilatus driverless&lt;/strong&gt;: Head of Perception [2018-2019]&lt;br&gt;
&lt;strong&gt;gotthard driverless&lt;/strong&gt;: Computer Vision [2017-2018]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;driverless-cars-in-action&#34;&gt;Driverless cars in action&lt;/h2&gt;
&lt;h3 id=&#34;pilatus-driverless-fs-germany-2019&#34;&gt;pilatus driverless @FS Germany 2019&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://youtu.be/OX8RSyF9mnw?list=PLtuNXpGOPQ_b5fejuHQrdE8PcW5kJ31F7&amp;amp;t=10079&#34; title=&#34;Acceleration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Acceleration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://youtu.be/gcnngFyWnFQ?list=PLtuNXpGOPQ_b5fejuHQrdE8PcW5kJ31F7&amp;amp;t=13068&#34; title=&#34;Trackdrive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trackdrive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://youtu.be/sxqkt_ydOkY?list=PLtuNXpGOPQ_b5fejuHQrdE8PcW5kJ31F7&amp;amp;t=10749&#34; title=&#34;AutoX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AutoX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gotthard-driverless-fs-germany-2018&#34;&gt;gotthard driverless @FS Germany 2018&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://youtu.be/HegmIXASKow?t=11694&#34; title=&#34;Trackdrive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trackdrive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;We would like to thank 
&lt;a href=&#34;http://amzracing.ch/&#34; title=&#34;AMZ&amp;#39;s Homepage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMZ Racing&lt;/a&gt;, especially the 
&lt;a href=&#34;http://driverless.amzracing.ch/&#34; title=&#34;AMZ-Driverless&amp;#39; Homepage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Driverless&lt;/a&gt; team without whom this project would not have been possible. The work is also supported by Toyota Motor Europe via the project 
&lt;a href=&#34;https://www.trace.ethz.ch/&#34; title=&#34;Trace-Zurich&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TRACE-Zurich&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AMZ Driverless: The Full Autonomous Racing System</title>
      <link>https://ankitdhall.github.io/publication/amz-driverless-full-autonomous-racing/</link>
      <pubDate>Wed, 08 May 2019 18:06:58 +0200</pubDate>
      <guid>https://ankitdhall.github.io/publication/amz-driverless-full-autonomous-racing/</guid>
      <description>&lt;p&gt;This paper presents the algorithms and system architecture of an autonomous racecar. The introduced vehicle is powered by a software stack designed for robustness, reliability, and extensibility. In order to autonomously race around a previously unknown track, the proposed solution combines state of the art techniques from different fields of robotics. Specifically, perception, estimation, and control are incorporated into one high-performance autonomous racecar. This complex robotic system, developed by AMZ Driverless and ETH Zurich, finished 1st overall at each competition we attended: Formula Student Germany 2017, Formula Student Italy 2018 and Formula Student Germany 2018. We discuss the findings and learnings from these competitions and present an experimental evaluation of each module of our solution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-time 3D Traffic Cone Detection for Autonomous Driving</title>
      <link>https://ankitdhall.github.io/publication/keypoint-network-amz/</link>
      <pubDate>Wed, 06 Feb 2019 10:24:27 +0200</pubDate>
      <guid>https://ankitdhall.github.io/publication/keypoint-network-amz/</guid>
      <description>&lt;p&gt;Considerable progress has been made in semantic scene understanding of road scenes with monocular cameras. It is, however, mainly focused on certain specific classes such as cars, bicyclists and pedestrians. This work investigates traffic cones, an object category crucial for traffic control in the context of autonomous vehicles. 3D object detection using images from a monocular camera is intrinsically an ill-posed problem. In this work, we exploit the unique structure of traffic cones and propose a pipelined approach to solve this problem. Specifically, we first detect cones in images by a modified 2D object detector. Following which the keypoints on a traffic cone are recognized with the help of our deep structural regression network, here, the fact that the cross-ratio is projection invariant is leveraged for network regularization. Finally, the 3D position of cones is recovered via the classical Perspective n-Point algorithm using correspondences obtained from the keypoint regression. Extensive experiments show that our approach can accurately detect traffic cones and estimate their position in the 3D world in real time. The proposed method is also deployed on a real-time, autonomous system. It runs efficiently on the low-power Jetson TX2, providing accurate 3D position estimates, allowing a race-car to map and drive autonomously on an unseen track indicated by traffic cones. With the help of robust and accurate perception, our race-car won both Formula Student Competitions held in Italy and Germany in 2018, cruising at a top speed of 54 km/h on our driverless platform ``gotthard driverless&amp;rsquo;&amp;rsquo;
&lt;img src=&#34;FSG19_Trackdrive_D.gif&#34; alt=&#34;alt text&#34; title=&#34;&amp;lt;&amp;lt;gotthard driverless&amp;gt;&amp;gt; on the Trackdrive at Formula Student Germany 2018&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-time 3D Pose Estimation with a Monocular Camera Using Deep Learning and Object Priors</title>
      <link>https://ankitdhall.github.io/publication/real-time-3d-monocular-pose-estimation/</link>
      <pubDate>Sat, 08 Sep 2018 18:07:26 +0200</pubDate>
      <guid>https://ankitdhall.github.io/publication/real-time-3d-monocular-pose-estimation/</guid>
      <description>&lt;p&gt;We propose a complete pipeline that allows object detection and simultaneously estimate the pose of these multiple object instances using just a single image. A novel &amp;ldquo;keypoint regression&amp;rdquo; scheme with a cross-ratio term is introduced that exploits prior information about the object&amp;rsquo;s shape and size to regress and find specific feature points. Further, a priori 3D information about the object is used to match 2D-3D correspondences and accurately estimate object positions up to a distance of 15m. A detailed discussion of the results and an in-depth analysis of the pipeline is presented. The pipeline runs efficiently on a low-powered Jetson TX2 and is deployed as part of the perception pipeline on a real-time autonomous vehicle cruising at a top speed of 54 km/hr.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convoluted Mixture of Deep Experts for Robust Semantic Segmentation</title>
      <link>https://ankitdhall.github.io/publication/cmode/</link>
      <pubDate>Tue, 05 Apr 2016 23:41:00 +0200</pubDate>
      <guid>https://ankitdhall.github.io/publication/cmode/</guid>
      <description>&lt;p&gt;In this paper, we propose Convoluted Mixture of Deep Experts (CMoDE) model that enables a multi-stream deep neural network architecture to learn features from complementary modalities and spectra that are resilient to commonly observed environmental disturbances. Some of these disturbances include shadows, snow, rain and glare which vary depending with season and time of the day. Our model first adaptively weighs features from each of the individual experts and then further learns fused representations that are robust to these disturbances. We comprehensively evaluate the CMoDE model against several other existing fusion approaches and show that our proposed model exceeds the state-of-the-art.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
