<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>code | Ankit Dhall</title>
    <link>https://ankitdhall.github.io/tags/code/</link>
      <atom:link href="https://ankitdhall.github.io/tags/code/index.xml" rel="self" type="application/rss+xml" />
    <description>code</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Ankit Dhall</copyright><lastBuildDate>Wed, 08 Apr 2020 11:06:28 +0200</lastBuildDate>
    <image>
      <url>https://ankitdhall.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>code</title>
      <link>https://ankitdhall.github.io/tags/code/</link>
    </image>
    
    <item>
      <title>Robust Semantic Segmentation</title>
      <link>https://ankitdhall.github.io/project/robust-semantic-segmentation/</link>
      <pubDate>Wed, 08 Apr 2020 11:06:28 +0200</pubDate>
      <guid>https://ankitdhall.github.io/project/robust-semantic-segmentation/</guid>
      <description>&lt;p&gt;The work was done in collaboration at 
&lt;a href=&#34;http://www2.informatik.uni-freiburg.de/~burgard&#34; title=&#34;Prof. Wolfram Burgard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Wolfram Burgard&amp;rsquo;s&lt;/a&gt; 
&lt;a href=&#34;http://ais.informatik.uni-freiburg.de/index_en.php&#34; title=&#34;AIS, Uni-Freiburg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Autonomous Intelligent Systems, University of Freiburg&lt;/a&gt; with 
&lt;a href=&#34;http://www2.informatik.uni-freiburg.de/~valada/&#34; title=&#34;Abhinav Valada&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abhinav Valada&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The project involves designing an innovative learnable mechanism to perform robust semantic segmentation in adverse conditions. The model learns how to dynamically weigh different modalities and trust them based on their quality resulting in improved segmentation. We also perform experiments on an autonomous car robot, Viona to drive in forested terrain based only on the perception input of the proposed CNN model.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;adapnet&#34;&gt;AdapNet&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;adapnet.gif&#34; alt=&#34;alt text&#34; title=&#34;Qualitative results under various adverse conditions like rain, nigth and fall scenarios from the dataset.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;icra17_adapnet1.gif&#34; alt=&#34;alt text&#34; title=&#34;AdapNet CNN architecture.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;code-and-data&#34;&gt;Code and Data&lt;/h3&gt;
&lt;p&gt;The code is available on 
&lt;a href=&#34;https://github.com/DeepSceneSeg/AdapNet&#34; title=&#34;AdapNet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and the multi-modal semantic segmentation image dataset used can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/#datasets&#34; title=&#34;Multi-modal semantic segmentation image dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The project demo can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34; title=&#34;Deep-Scene&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;unimodal_vs_cmode.png&#34; alt=&#34;alt text&#34; title=&#34;Comparing performance of uni-modal input to multi-modal input. AdapNet learns better by fusing smartly information across modalities&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;videos-and-demos&#34;&gt;Videos and Demos&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=E6gij6IS8n0&#34; title=&#34;AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=7teAwVMTCho&#34; title=&#34;Performance on Cityscapes dataset&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; Performance on Cityscapes dataset&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=cV-k0gkVfuY&#34; title=&#34;Off-road autonomous driving experiments&#34; target=&#34;_blank&#34;&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; Off-road autonomous driving experiments&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;autonomous_exp.png&#34; alt=&#34;alt text&#34; title=&#34;Autonomous driving in off-road terrain in adverse conditions. Perception based solely on semantic segmentation output from CMoDE.&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;cmode&#34;&gt;CMoDE&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;qual-results.png&#34; alt=&#34;alt text&#34; title=&#34;Qualitative results with RGB and EVI/Depth as input and their corresponding weights that the CMoDE generates and uses based on image quality to produced improved semantic semgentation as compared to an agnostic model (LFC).&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;segmentation-video.gif&#34; alt=&#34;alt text&#34; title=&#34;Autonomous driving in off-road terrain in adverse conditions. Perception based solely on semantic segmentation output from CMoDE.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;qual_results2.png&#34; alt=&#34;alt text&#34; title=&#34;Qualitative results in different conditions.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;code-and-data-1&#34;&gt;Code and Data&lt;/h3&gt;
&lt;p&gt;The code is available on 
&lt;a href=&#34;https://github.com/DeepSceneSeg/CMoDE&#34; title=&#34;CMoDE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and the multi-modal semantic segmentation image dataset used can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/#datasets&#34; title=&#34;Multi-modal semantic segmentation image dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The official project page can be found 
&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34; title=&#34;Deep-Scene&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Convoluted Mixture of Deep Experts for Robust Semantic Segmentation</title>
      <link>https://ankitdhall.github.io/publication/cmode/</link>
      <pubDate>Tue, 05 Apr 2016 23:41:00 +0200</pubDate>
      <guid>https://ankitdhall.github.io/publication/cmode/</guid>
      <description>&lt;p&gt;In this paper, we propose Convoluted Mixture of Deep Experts (CMoDE) model that enables a multi-stream deep neural network architecture to learn features from complementary modalities and spectra that are resilient to commonly observed environmental disturbances. Some of these disturbances include shadows, snow, rain and glare which vary depending with season and time of the day. Our model first adaptively weighs features from each of the individual experts and then further learns fused representations that are robust to these disturbances. We comprehensively evaluate the CMoDE model against several other existing fusion approaches and show that our proposed model exceeds the state-of-the-art.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
